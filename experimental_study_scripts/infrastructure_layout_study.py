# infrastructure_layout_study.py
"""
Infrastructure Layout Sensitivity Study

Research Question: Do operational regime boundaries depend on specific restaurant 
layouts, or are they robust across different spatial configurations?

Test: Multiple infrastructure seeds (different random restaurant layouts) with 
same structural parameters (10km Ã— 10km, 10 restaurants).

Design: 3 seeds Ã— 3 critical ratios Ã— 2 scales = 18 design points
Total runs: 18 Ã— 5 replications = 90 runs (~6-7 minutes)
"""

# %% CELL 1: Enable Autoreload
%load_ext autoreload 
%autoreload 2

# %% CELL 2: Setup and Imports
import sys
import os
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from delivery_sim.simulation.configuration import (
    StructuralConfig, OperationalConfig, ExperimentConfig, 
    LoggingConfig, ScoringConfig
)
from delivery_sim.infrastructure.infrastructure import Infrastructure
from delivery_sim.infrastructure.infrastructure_analyzer import InfrastructureAnalyzer
from delivery_sim.experimental.design_point import DesignPoint
from delivery_sim.experimental.experimental_runner import ExperimentalRunner
from delivery_sim.utils.logging_system import configure_logging

print("="*80)
print("INFRASTRUCTURE LAYOUT SENSITIVITY STUDY")
print("="*80)
print("Research Question: Do regime boundaries hold across different layouts?")

# %% CELL 3: Logging Configuration
logging_config = LoggingConfig(
    console_level="INFO",
    component_levels={
        "services": "ERROR",
        "entities": "ERROR",
        "repositories": "ERROR",
        "utils": "ERROR",
        "system_data": "ERROR",
        "simulation.runner": "INFO",
        "infrastructure": "INFO",
        "experimental.runner": "INFO",
    }
)
configure_logging(logging_config)
print("âœ“ Logging configured")

# %% CELL 3.5: Research Question
"""
Document research question and its evolution.
"""

print("\n" + "="*80)
print("RESEARCH QUESTION")
print("="*80)

# ==============================================================================
# MAIN RESEARCH QUESTION
# ==============================================================================
research_question = """
Do operational regime boundaries depend on specific restaurant layouts,
or are they robust across different spatial configurations?

Test: Three different random layouts (seeds 42, 100, 200) with same 
structural parameters (10km Ã— 10km, 10 restaurants).

Critical ratios: 3.5 (stable), 5.0 (volatile), 7.0 (failure) based on
arrival_interval_ratio_study.py findings.

If boundaries shift significantly across layouts â†’ spatial configuration matters
If boundaries hold consistently â†’ ratio determines regime independent of layout
"""

# ==============================================================================
# CONTEXT & MOTIVATION
# ==============================================================================
context = """
Previous study (arrival_interval_ratio_study.py) identified operational regime 
transitions around arrival interval ratios 6.0-7.0 using a single fixed 
infrastructure (seed=42).

Validity concern: Are these boundaries specific to that particular restaurant 
layout, or do they represent general principles?

This study tests layout sensitivity by varying only the structural seed while
keeping all structural parameters (area size, restaurant count) identical.

Evolution from broad exploration:
1. Initially: "How does infrastructure affect system performance?" (too vague)
2. Refined: "What aspect of infrastructure - size, density, or layout?" 
3. Current: Start minimal - test layout variation only
4. Future: If layout matters, vary area_size and num_restaurants systematically
"""

# ==============================================================================
# SUB-QUESTIONS & HYPOTHESES
# ==============================================================================
sub_questions = """
Layout robustness hypothesis predicts:

1. Regime boundaries consistent across layouts
   - All three seeds show stable regime at ratio 3.5
   - All three seeds show volatile regime at ratio 5.0
   - All three seeds show failure regime at ratio 7.0

2. Performance levels may vary, but regime classification holds
   - Different layouts may have different typical_distance
   - Absolute assignment times may differ proportionally
   - But qualitative behavior (stable/volatile/failure) should match

3. Scale validation still holds within each layout
   - Baseline vs 2Ã— baseline should show similar patterns
   - Within each seed, ratio determines regime

Falsification scenarios:
- If seed 42 fails at ratio 7.0 but seed 100 remains stable â†’ layout matters critically
- If typical_distance doesn't predict performance variation â†’ need other spatial metrics
"""

# ==============================================================================
# SCOPE & BOUNDARIES
# ==============================================================================
scope = """
Fixed factors:
- Area size: 10km Ã— 10km (same as previous study)
- Restaurant count: 10 (same as previous study)
- Driver speed: 0.5 km/min
- Pairing: Disabled throughout
- Service duration: Fixed distribution (mean=100, std=60, min=30, max=200)

Varied factors:
- Structural seeds: [42, 100, 200] â†’ three different restaurant layouts
- Arrival interval ratios: [3.5, 5.0, 7.0] â†’ critical regime boundaries
- Operational scale: Baseline vs 2Ã— baseline for each ratio

Not varied in this study (future work):
- Area size (affects geographical scale)
- Restaurant count (affects density)
- Pairing configurations

Design: 3 seeds Ã— 3 ratios Ã— 2 scales = 18 design points Ã— 5 replications = 90 runs
"""

# ==============================================================================
# KEY METRICS & ANALYSIS FOCUS
# ==============================================================================
analysis_focus = """
Primary comparison: For each ratio, compare across three seeds

Metrics:
1. Assignment time: Mean and variability
2. Completion rate: System stability indicator
3. Typical distance: Infrastructure characteristic (explains performance differences?)

Analysis approach:
- Group by ratio, compare across seeds
- Check if regime classification (stable/volatile/failure) holds
- Examine if typical_distance predicts performance variation
- Within each seed, verify baseline vs 2Ã— baseline validation

Expected patterns:
- Regime boundaries at same ratios regardless of layout
- Absolute performance may differ by typical_distance
- If patterns diverge significantly â†’ layout has structural effects beyond distance
"""

print(research_question)
print("\n" + "-"*80)
print("CONTEXT & MOTIVATION")
print("-"*80)
print(context)
print("\n" + "-"*80)
print("SUB-QUESTIONS & HYPOTHESES")
print("-"*80)
print(sub_questions)
print("\n" + "-"*80)
print("SCOPE & BOUNDARIES")
print("-"*80)
print(scope)
print("\n" + "-"*80)
print("KEY METRICS & ANALYSIS FOCUS")
print("-"*80)
print(analysis_focus)
print("\n" + "="*80)
print("âœ“ Research question documented - reference this to guide analysis decisions")
print("="*80)

# %% CELL 4: Infrastructure Configuration(s)
"""
INFRASTRUCTURE STUDY: Testing layout sensitivity with multiple structural seeds.

Same structural parameters, different random layouts.
"""

infrastructure_configs = [
    {
        'name': 'area_10km',
        'config': StructuralConfig(
            delivery_area_size=10,
            num_restaurants=10,
            driver_speed=0.5
        )
    }
]

print(f"âœ“ Defined {len(infrastructure_configs)} infrastructure configuration(s)")
for config in infrastructure_configs:
    print(f"  â€¢ {config['name']}: {config['config']}")

# %% CELL 5: Structural Seeds
"""
CRITICAL CHANGE: Test multiple structural seeds for layout sensitivity.

Previous study used single seed (42).
This study tests three different layouts with identical structural parameters.
"""

structural_seeds = [42, 100, 200]

print(f"âœ“ Structural seeds: {structural_seeds}")
print(f"âœ“ Testing layout sensitivity with {len(structural_seeds)} different restaurant configurations")

# %% CELL 6: Create Infrastructure Instances
"""
Create and analyze infrastructure instances.
Store analyzer for reuse in visualization.
"""

infrastructure_instances = []

print("\n" + "="*50)
print("INFRASTRUCTURE INSTANCES CREATION")
print("="*50)

for infra_config in infrastructure_configs:
    for structural_seed in structural_seeds:
        
        # Create infrastructure instance
        instance_name = f"{infra_config['name']}_seed{structural_seed}"
        print(f"\nðŸ“ Creating infrastructure: {instance_name}")
        
        infrastructure = Infrastructure(
            infra_config['config'],
            structural_seed
        )
        
        # Analyze infrastructure
        analyzer = InfrastructureAnalyzer(infrastructure)
        analysis_results = analyzer.analyze_complete_infrastructure()
        
        # Store instance with metadata
        infrastructure_instances.append({
            'name': instance_name,
            'infrastructure': infrastructure,
            'analyzer': analyzer,  # Store analyzer for reuse
            'analysis': analysis_results,
            'config_name': infra_config['name'],
            'seed': structural_seed
        })
        
        print(f"  âœ“ Infrastructure created and analyzed")
        print(f"    â€¢ Typical distance: {analysis_results['typical_distance']:.3f}km")
        print(f"    â€¢ Restaurant density: {analysis_results['restaurant_density']:.4f}/kmÂ²")

print(f"\n{'='*50}")
print(f"âœ“ Created {len(infrastructure_instances)} infrastructure instance(s)")
print(f"âœ“ Breakdown: {len(infrastructure_configs)} configs Ã— {len(structural_seeds)} seeds")
print(f"{'='*50}")

# Display typical distance comparison
print("\nðŸ“Š Infrastructure Comparison:")
for instance in infrastructure_instances:
    print(f"  {instance['name']:25s}: typical_distance={instance['analysis']['typical_distance']:.3f}km")

# %% CELL 6.5: Visualize Restaurant Layouts
"""
OPTIONAL: Visualize infrastructure instances to understand layout differences.

For layout sensitivity study, this is particularly valuable to see how
restaurant spatial configurations differ across seeds.
"""

print("\n" + "="*50)
print("INFRASTRUCTURE LAYOUT VISUALIZATION")
print("="*50)

import matplotlib.pyplot as plt

print(f"\nVisualizing {len(infrastructure_instances)} different restaurant layouts...")
print("Compare spatial patterns to understand what 'layout sensitivity' means.\n")

for instance in infrastructure_instances:
    print(f"{'='*50}")
    print(f"Layout: {instance['name']}")
    print(f"Typical Distance: {instance['analysis']['typical_distance']:.3f}km")
    print(f"Restaurant Density: {instance['analysis']['restaurant_density']:.4f}/kmÂ²")
    print(f"{'='*50}")
    
    # Visualize using stored analyzer
    instance['analyzer'].visualize_infrastructure()
    
    # Add custom header to distinguish layouts
    fig = plt.gcf()  # Get current figure
    seed = instance['seed']
    typical_dist = instance['analysis']['typical_distance']
    
    # Create informative title
    custom_title = (f"Infrastructure Layout: Seed {seed}\n"
                   f"Typical Distance: {typical_dist:.3f}km | "
                   f"Area: 10Ã—10km | Restaurants: 10")
    
    fig.suptitle(custom_title, fontsize=14, fontweight='bold', y=0.98)
    
    plt.tight_layout()
    plt.show()
    
    print(f"âœ“ {instance['name']} visualized\n")

print(f"{'='*50}")
print("âœ“ All layouts visualized")
print("âœ“ Compare spatial patterns:")
print("  - Are restaurants clustered or dispersed?")
print("  - Do some layouts have outlier restaurants?")
print("  - How might these patterns affect delivery performance?")
print(f"{'='*50}")

# %% CELL 7: Scoring Configuration(s)
"""
Single baseline scoring configuration for this study.
"""

scoring_configs = [
    {
        'name': 'baseline',
        'config': ScoringConfig()  # Use defaults
    }
]

print(f"\nâœ“ Defined {len(scoring_configs)} scoring configuration(s)")
for config in scoring_configs:
    print(f"  â€¢ {config['name']}")

# %% CELL 8: Operational Configuration(s)
"""
OPERATIONAL STUDY: Focus on critical ratios that define regime boundaries.

Reduced from 10 ratios to 3 critical ratios based on previous study findings:
- 3.5: Stable regime
- 5.0: Volatile regime  
- 7.0: Failure regime

For each ratio, create validation pair (baseline + 2Ã— baseline).
"""

# CRITICAL CHANGE: Focus on regime boundary ratios only
target_arrival_interval_ratios = [3.5, 5.0, 7.0]

# Fixed pairing configuration
FIXED_PAIRING_CONFIG = {
    'pairing_enabled': False,
    'restaurants_proximity_threshold': None,
    'customers_proximity_threshold': None
}

# Fixed service duration configuration
FIXED_SERVICE_CONFIG = {
    'mean_service_duration': 100,
    'service_duration_std_dev': 60,
    'min_service_duration': 30,
    'max_service_duration': 200
}

# Build operational configs
operational_configs = []

for ratio in target_arrival_interval_ratios:
    # Baseline configuration: (1.0, ratio)
    operational_configs.append({
        'name': f'ratio_{ratio:.1f}_baseline',
        'config': OperationalConfig(
            mean_order_inter_arrival_time=1.0,
            mean_driver_inter_arrival_time=ratio,
            **FIXED_PAIRING_CONFIG,
            **FIXED_SERVICE_CONFIG
        )
    })
    
    # 2x Baseline configuration: (2.0, 2Ã—ratio)
    operational_configs.append({
        'name': f'ratio_{ratio:.1f}_2x_baseline',
        'config': OperationalConfig(
            mean_order_inter_arrival_time=2.0,
            mean_driver_inter_arrival_time=2.0 * ratio,
            **FIXED_PAIRING_CONFIG,
            **FIXED_SERVICE_CONFIG
        )
    })

print(f"âœ“ Defined {len(operational_configs)} operational configurations")
print(f"âœ“ Testing {len(target_arrival_interval_ratios)} critical arrival interval ratios")
print(f"âœ“ Each ratio has 2 validation pairs (baseline + 2Ã— baseline)")

# Display configurations
for config in operational_configs:
    op_config = config['config']
    ratio = op_config.mean_driver_inter_arrival_time / op_config.mean_order_inter_arrival_time
    print(f"  â€¢ {config['name']}: "
          f"order_interval={op_config.mean_order_inter_arrival_time:.1f}min, "
          f"driver_interval={op_config.mean_driver_inter_arrival_time:.1f}min, "
          f"ratio={ratio:.1f}")

# %% CELL 9: Design Point Creation
"""
Create design points from all combinations.

Design: 3 seeds Ã— 3 ratios Ã— 2 scales = 18 design points
"""

design_points = {}

print("\n" + "="*50)
print("DESIGN POINTS CREATION")
print("="*50)

for infra_instance in infrastructure_instances:
    for op_config in operational_configs:
        for scoring_config_dict in scoring_configs:
            
            # Generate design point name including seed info
            design_name = f"{infra_instance['name']}_{op_config['name']}"
            
            # Create design point
            design_points[design_name] = DesignPoint(
                infrastructure=infra_instance['infrastructure'],
                operational_config=op_config['config'],
                scoring_config=scoring_config_dict['config'],
                name=design_name
            )
            
            print(f"  âœ“ Design point: {design_name}")

print(f"\n{'='*50}")
print(f"âœ“ Created {len(design_points)} design points")
print(f"âœ“ Breakdown: {len(infrastructure_instances)} infra Ã— "
      f"{len(operational_configs)} operational Ã— {len(scoring_configs)} scoring")
print(f"{'='*50}")

# %% CELL 10: Experiment Configuration
experiment_config = ExperimentConfig(
    simulation_duration=2000,
    num_replications=5,
    operational_master_seed=100,
    collection_interval=1.0
)

total_runs = len(design_points) * experiment_config.num_replications
estimated_time = total_runs * 5

print(f"âœ“ Experiment configuration:")
print(f"  â€¢ Simulation duration: {experiment_config.simulation_duration} minutes")
print(f"  â€¢ Replications per design point: {experiment_config.num_replications}")
print(f"  â€¢ Operational master seed: {experiment_config.operational_master_seed}")
print(f"  â€¢ Collection interval: {experiment_config.collection_interval} minutes")
print(f"\nâœ“ Execution plan:")
print(f"  â€¢ Total simulation runs: {total_runs}")
print(f"  â€¢ Estimated time: ~{estimated_time:.0f} seconds (~{estimated_time/60:.1f} minutes)")

# %% CELL 11: Execute Experimental Study
"""
Run all design points through the experimental runner.
"""

print("\n" + "="*50)
print("EXPERIMENTAL EXECUTION")
print("="*50)

runner = ExperimentalRunner()
study_results = runner.run_experimental_study(design_points, experiment_config)

print(f"\nâœ… INFRASTRUCTURE LAYOUT STUDY COMPLETE!")
print(f"âœ“ Executed {len(study_results)} design points")
print(f"âœ“ Each with {experiment_config.num_replications} replications")

# %% CELL 12: Time Series Data Processing for Warmup Analysis
print("\n" + "="*50)
print("TIME SERIES DATA PROCESSING FOR WARMUP ANALYSIS")
print("="*50)

from delivery_sim.warmup_analysis.time_series_processing import extract_warmup_time_series

print("Processing time series data for warmup detection...")

all_time_series_data = extract_warmup_time_series(
    study_results=study_results,
    design_points=design_points,
    metrics=['active_drivers', 'available_drivers', 'unassigned_delivery_entities'],  # â† ADDED available_drivers
    moving_average_window=100
)

print(f"âœ“ Time series processing complete for {len(all_time_series_data)} design points")
print(f"âœ“ Metrics extracted: active_drivers, available_drivers, unassigned_delivery_entities")  # â† UPDATED
print(f"âœ“ Ready for warmup analysis visualization")

# %% CELL 13: Warmup Analysis Visualization
print("\n" + "="*50)
print("WARMUP ANALYSIS VISUALIZATION")
print("="*50)

from delivery_sim.warmup_analysis.visualization import WelchMethodVisualization
import matplotlib.pyplot as plt

print("Creating warmup analysis plots...")

# Initialize visualization
viz = WelchMethodVisualization(figsize=(16, 10))

# Group design points by seed for organized display
seed_groups = {}
for design_name in all_time_series_data.keys():
    # Extract seed from design name (e.g., "area_10km_seed42_ratio_3.5_baseline")
    parts = design_name.split('_')
    seed_str = parts[2].replace('seed', '')  # "42"
    seed = int(seed_str)
    
    if seed not in seed_groups:
        seed_groups[seed] = []
    seed_groups[seed].append(design_name)

print(f"âœ“ Grouped {len(all_time_series_data)} design points by {len(seed_groups)} seeds")

# Create plots systematically by seed
plot_count = 0
for seed in sorted(seed_groups.keys()):
    print(f"\nSeed {seed}:")
    
    for design_name in sorted(seed_groups[seed]):
        plot_title = f"Warmup Analysis: {design_name}"
        time_series_data = all_time_series_data[design_name]
        fig = viz.create_warmup_analysis_plot(time_series_data, title=plot_title)
        
        plt.show()
        print(f"    âœ“ {design_name} plot displayed")
        plot_count += 1

print(f"\nâœ“ Warmup analysis visualization complete")
print(f"âœ“ Created {plot_count} warmup analysis plots")
print(f"âœ“ Organized by {len(seed_groups)} seeds")

# %% CELL 14: Warmup Period Determination
print("\n" + "="*50)
print("WARMUP PERIOD DETERMINATION")
print("="*50)

# Set warmup period based on visual inspection of Cell 13 plots
uniform_warmup_period = 500  # UPDATE THIS based on visual inspection

print(f"âœ“ Warmup period set: {uniform_warmup_period} minutes")
print(f"âœ“ Based on visual inspection of active drivers oscillation around Little's Law values")
print(f"âœ“ Analysis window: {experiment_config.simulation_duration - uniform_warmup_period} minutes of post-warmup data")

# %% CELL 15: Process Through Analysis Pipeline
print("\n" + "="*80)
print("PROCESSING THROUGH ANALYSIS PIPELINE")
print("="*80)

from delivery_sim.analysis_pipeline.pipeline_coordinator import ExperimentAnalysisPipeline

# Initialize pipeline with delivery_unit_metrics enabled
pipeline = ExperimentAnalysisPipeline(
    warmup_period=uniform_warmup_period,
    enabled_metric_types=['order_metrics', 'system_metrics', 'delivery_unit_metrics', 'system_state_metrics'],  # â† ADDED
    confidence_level=0.95
)

# Process each design point
design_analysis_results = {}

print(f"\nProcessing {len(study_results)} design points...")
print(f"Warmup period: {uniform_warmup_period} minutes")
print(f"Confidence level: 95%\n")

for i, (design_name, replication_results) in enumerate(study_results.items(), 1):
    print(f"[{i:2d}/{len(study_results)}] Analyzing {design_name}...")
    
    analysis_result = pipeline.analyze_experiment(replication_results)
    design_analysis_results[design_name] = analysis_result
    
    print(f"    âœ“ Processed {analysis_result['num_replications']} replications")

print(f"\nâœ“ Analysis pipeline complete for all {len(design_analysis_results)} design points")
print(f"âœ“ Results stored in 'design_analysis_results'")
# %% CELL 16: Extract and Present Key Metrics (TABLE FORMAT WITH SEED)
print("\n" + "="*80)
print("KEY PERFORMANCE METRICS EXTRACTION AND PRESENTATION")
print("="*80)

import re

def extract_seed_ratio_and_type(design_name):
    """Extract seed, ratio, and interval type from design point name."""
    # Pattern: area_10km_seed42_ratio_3.5_baseline
    match = re.match(r'.*_seed(\d+)_ratio_([\d.]+)_(baseline|2x_baseline)', design_name)
    if match:
        seed = int(match.group(1))
        ratio = float(match.group(2))
        interval_type = match.group(3)
        return seed, ratio, interval_type
    return None, None, None

# Extract comprehensive metrics
metrics_data = []

for design_name, analysis_result in design_analysis_results.items():
    seed, ratio, interval_type = extract_seed_ratio_and_type(design_name)
    if seed is None:
        continue
    
    stats_with_cis = analysis_result.get('statistics_with_cis', {})
    
    # Extract assignment time (from order_metrics)
    order_metrics = stats_with_cis.get('order_metrics', {})
    assignment_time = order_metrics.get('assignment_time', {})
    
    mean_of_means = assignment_time.get('mean_of_means', {})
    mom_estimate = mean_of_means.get('point_estimate', 0)
    mom_ci = mean_of_means.get('confidence_interval', [0, 0])
    mom_ci_width = (mom_ci[1] - mom_ci[0]) / 2 if mom_ci[0] is not None else 0
    
    std_of_means = assignment_time.get('std_of_means', {})
    som_estimate = std_of_means.get('point_estimate', 0)
    
    mean_of_stds = assignment_time.get('mean_of_stds', {})
    mos_estimate = mean_of_stds.get('point_estimate', 0)
    
    # Extract completion rate (from system_metrics)
    system_metrics = stats_with_cis.get('system_metrics', {})
    completion_rate = system_metrics.get('system_completion_rate', {})
    
    comp_estimate = completion_rate.get('point_estimate', 0)
    comp_ci = completion_rate.get('confidence_interval', [0, 0])
    comp_ci_width = (comp_ci[1] - comp_ci[0]) / 2 if comp_ci[0] is not None else 0
    
    # Extract total distance (from delivery_unit_metrics)
    delivery_unit_metrics = stats_with_cis.get('delivery_unit_metrics', {})
    total_distance_stats = delivery_unit_metrics.get('total_distance', {})
    
    distance_mean_of_means = total_distance_stats.get('mean_of_means', {})
    distance_estimate = distance_mean_of_means.get('point_estimate', 0)
    distance_ci = distance_mean_of_means.get('confidence_interval', [0, 0])
    distance_ci_width = (distance_ci[1] - distance_ci[0]) / 2 if distance_ci[0] is not None else 0
    
    # Extract available_drivers (from system_state_metrics)
    state_metrics = stats_with_cis.get('system_state_metrics', {})
    available_drivers_stats = state_metrics.get('available_drivers', {})
    
    avail_mean_of_means = available_drivers_stats.get('mean_of_means', {})
    avail_mom_estimate = avail_mean_of_means.get('point_estimate', 0)
    avail_mom_ci = avail_mean_of_means.get('confidence_interval', [0, 0])
    avail_mom_ci_width = (avail_mom_ci[1] - avail_mom_ci[0]) / 2 if avail_mom_ci[0] is not None else 0
    
    avail_std_of_means = available_drivers_stats.get('std_of_means', {})
    avail_som_estimate = avail_std_of_means.get('point_estimate', 0)
    
    avail_mean_of_stds = available_drivers_stats.get('mean_of_stds', {})
    avail_mos_estimate = avail_mean_of_stds.get('point_estimate', 0)
    
    metrics_data.append({
        'seed': seed,
        'ratio': ratio,
        'interval_type': interval_type,
        'mom_estimate': mom_estimate,
        'mom_ci_width': mom_ci_width,
        'som_estimate': som_estimate,
        'mos_estimate': mos_estimate,
        'comp_estimate': comp_estimate,
        'comp_ci_width': comp_ci_width,
        'distance_estimate': distance_estimate,
        'distance_ci_width': distance_ci_width,
        'avail_mom_estimate': avail_mom_estimate,
        'avail_mom_ci_width': avail_mom_ci_width,
        'avail_som_estimate': avail_som_estimate,
        'avail_mos_estimate': avail_mos_estimate,
    })

# Sort by ratio, then seed, then interval type
metrics_data.sort(key=lambda x: (x['ratio'], x['seed'], x['interval_type']))

# ========================================
# VIEW 1: Complete table sorted by ratio
# ========================================
print("\nðŸŽ¯ KEY PERFORMANCE METRICS: WITH AVAILABLE DRIVERS")
print("="*220)
print("  Seed   Ratio    Interval        Mean of Means     Std of    Mean of           Completion Rate      Mean Total Distance      Available Drivers (mean_of_means)    Avail     Avail")
print("                      Type    (Assignment Time)      Means       Stds             (with 95% CI)            (with 95% CI)                  (with 95% CI)          Std of   Mean of")
print("                                                                                                                                                                     Means     Stds")
print("="*220)

for row in metrics_data:
    seed = row['seed']
    ratio = row['ratio']
    interval_display = "2x Baseline" if row['interval_type'] == '2x_baseline' else "Baseline"
    
    mom_str = f"{row['mom_estimate']:5.2f} Â± {row['mom_ci_width']:5.2f}"
    som_str = f"{row['som_estimate']:5.2f}"
    mos_str = f"{row['mos_estimate']:5.2f}"
    comp_str = f"{row['comp_estimate']:.3f} Â± {row['comp_ci_width']:.3f}"
    dist_str = f"{row['distance_estimate']:5.2f} Â± {row['distance_ci_width']:5.2f}"
    avail_mom_str = f"{row['avail_mom_estimate']:5.2f} Â± {row['avail_mom_ci_width']:5.2f}"
    avail_som_str = f"{row['avail_som_estimate']:5.2f}"
    avail_mos_str = f"{row['avail_mos_estimate']:5.2f}"
    
    print(f"  {seed:4d}   {ratio:3.1f}  {interval_display:12s}     {mom_str:>16s}    {som_str:>7s}    {mos_str:>7s}          {comp_str:>15s}          {dist_str:>18s}          {avail_mom_str:>22s}    {avail_som_str:>7s}   {avail_mos_str:>7s}")

print("="*220)

# ========================================
# VIEW 2: Grouped by seed
# ========================================
print("\n\nAlternative view grouped by seed:")
print("ðŸŽ¯ LAYOUT-SPECIFIC PERFORMANCE")
print("="*220)

for seed in sorted(set(row['seed'] for row in metrics_data)):
    # Get typical distance for this seed
    instance_name = f"area_10km_seed{seed}"
    typical_dist = next(i['analysis']['typical_distance'] for i in infrastructure_instances if i['name'] == instance_name)
    
    print(f"\nSeed {seed} (typical_distance={typical_dist:.3f}km):")
    print("-"*220)
    print(" Ratio    Interval        Mean of Means     Std of    Mean of           Completion Rate      Mean Total Distance      Available Drivers (mean_of_means)    Avail     Avail")
    print("              Type    (Assignment Time)      Means       Stds             (with 95% CI)            (with 95% CI)                  (with 95% CI)          Std of   Mean of")
    print("                                                                                                                                                              Means     Stds")
    print("-"*220)
    
    seed_data = [row for row in metrics_data if row['seed'] == seed]
    for row in seed_data:
        ratio = row['ratio']
        interval_display = "2x Baseline" if row['interval_type'] == '2x_baseline' else "Baseline"
        
        mom_str = f"{row['mom_estimate']:5.2f} Â± {row['mom_ci_width']:5.2f}"
        som_str = f"{row['som_estimate']:5.2f}"
        mos_str = f"{row['mos_estimate']:5.2f}"
        comp_str = f"{row['comp_estimate']:.3f} Â± {row['comp_ci_width']:.3f}"
        dist_str = f"{row['distance_estimate']:5.2f} Â± {row['distance_ci_width']:5.2f}"
        avail_mom_str = f"{row['avail_mom_estimate']:5.2f} Â± {row['avail_mom_ci_width']:5.2f}"
        avail_som_str = f"{row['avail_som_estimate']:5.2f}"
        avail_mos_str = f"{row['avail_mos_estimate']:5.2f}"
        
        print(f"  {ratio:3.1f}  {interval_display:12s}     {mom_str:>16s}    {som_str:>7s}    {mos_str:>7s}          {comp_str:>15s}          {dist_str:>18s}          {avail_mom_str:>22s}    {avail_som_str:>7s}   {avail_mos_str:>7s}")

# ========================================
# VIEW 3: Grouped by ratio
# ========================================
print("\n\nAlternative view grouped by ratio:")
print("ðŸŽ¯ PERFORMANCE COMPARISON ACROSS LAYOUTS")
print("="*220)

for ratio in sorted(set(row['ratio'] for row in metrics_data)):
    print(f"\nRatio {ratio:.1f} - Comparing layouts:")
    print("-"*220)
    print("  Seed   Interval        Mean of Means     Std of    Mean of           Completion Rate      Mean Total Distance      Available Drivers (mean_of_means)    Avail     Avail")
    print("             Type    (Assignment Time)      Means       Stds             (with 95% CI)            (with 95% CI)                  (with 95% CI)          Std of   Mean of")
    print("                                                                                                                                                              Means     Stds")
    print("-"*220)
    
    ratio_data = [row for row in metrics_data if row['ratio'] == ratio]
    for row in ratio_data:
        seed = row['seed']
        interval_display = "2x Baseline" if row['interval_type'] == '2x_baseline' else "Baseline"
        
        mom_str = f"{row['mom_estimate']:5.2f} Â± {row['mom_ci_width']:5.2f}"
        som_str = f"{row['som_estimate']:5.2f}"
        mos_str = f"{row['mos_estimate']:5.2f}"
        comp_str = f"{row['comp_estimate']:.3f} Â± {row['comp_ci_width']:.3f}"
        dist_str = f"{row['distance_estimate']:5.2f} Â± {row['distance_ci_width']:5.2f}"
        avail_mom_str = f"{row['avail_mom_estimate']:5.2f} Â± {row['avail_mom_ci_width']:5.2f}"
        avail_som_str = f"{row['avail_som_estimate']:5.2f}"
        avail_mos_str = f"{row['avail_mos_estimate']:5.2f}"
        
        print(f"  {seed:4d}  {interval_display:12s}     {mom_str:>16s}    {som_str:>7s}    {mos_str:>7s}          {comp_str:>15s}          {dist_str:>18s}          {avail_mom_str:>22s}    {avail_som_str:>7s}   {avail_mos_str:>7s}")

print("\n" + "="*220)
print("âœ“ Metric extraction complete with layout sensitivity analysis, distance data, and available drivers")


# %% CELL 17: [APPENDIX] Single Replication Analysis - Testing Complementary Hypothesis
"""
EXPLORATORY ANALYSIS: Test hypothesis that available_drivers and unassigned_delivery_entities
should be complementary in individual replications.

Background: Cross-replication averaging in warmup plots shows both metrics positive simultaneously.
This is expected because different replications are in different stochastic phases.

Hypothesis: In individual replications, the two metrics should be more complementary
(when one is high, the other should be low).

Test cases:
- Ratio 5.0 (volatile regime) - expect some complementary behavior with oscillations
- Ratio 7.0 (failure regime) - expect available_drivers â‰ˆ 0 always
"""

print("\n" + "="*80)
print("APPENDIX: SINGLE REPLICATION TIME SERIES ANALYSIS")
print("="*80)

from delivery_sim.visualization.time_series_plots import TimeSeriesVisualizer
import matplotlib.pyplot as plt

# Initialize visualizer
viz = TimeSeriesVisualizer(figsize=(16, 12))

# ========================================
# Test Case 1: Ratio 5.0, Seed 200, Baseline
# ========================================
print("\n" + "="*80)
print("TEST CASE 1: Ratio 5.0 - Volatile Regime (Seed 200, Baseline)")
print("="*80)

design_name_50 = 'area_10km_seed200_ratio_5.0_baseline'
replication_results_50 = study_results[design_name_50]

print(f"\nDesign point: {design_name_50}")
print(f"Total replications available: {len(replication_results_50)}")

# Plot first 3 replications individually
print("\n--- Individual Replication Analysis ---")
for rep_idx in range(min(3, len(replication_results_50))):
    print(f"\nReplication {rep_idx + 1}:")
    
    single_rep_snapshots = replication_results_50[rep_idx]['system_snapshots']
    print(f"  â€¢ Total snapshots: {len(single_rep_snapshots)}")
    
    # Plot 1: Standard stacked view
    fig, axes = viz.plot_single_replication(
        single_rep_snapshots,
        metrics=['available_drivers', 'unassigned_delivery_entities', 'active_drivers'],
        title=f'{design_name_50} - Replication {rep_idx + 1} (Stacked View)',
        apply_smoothing=False,  # Show raw data first
        window=100
    )
    plt.show()
    
    # Plot 2: Complementary analysis view (dual y-axis)
    fig, axes = viz.plot_complementary_analysis(
        single_rep_snapshots,
        metric_pairs=[
            ('available_drivers', 'unassigned_delivery_entities')
        ],
        title=f'{design_name_50} - Rep {rep_idx + 1}: Complementary Dynamics Test'
    )
    plt.show()
    
    print(f"  âœ“ Replication {rep_idx + 1} plotted")

# Plot all replications overlaid for comparison
print("\n--- All Replications Overlaid ---")
fig, axes = viz.plot_multiple_replications(
    replication_results_50,
    metrics=['available_drivers', 'unassigned_delivery_entities'],
    design_name=design_name_50,
    show_average=True,
    apply_smoothing=False  # Show raw variability
)
plt.show()

print(f"\nâœ“ Ratio 5.0 analysis complete")
print("Observations:")
print("  â€¢ Do individual replications show complementary patterns?")
print("  â€¢ How much do replications differ from each other?")
print("  â€¢ Does the average (black line) look different from individuals?")

# ========================================
# Test Case 2: Ratio 7.0, Seed 200, Baseline
# ========================================
print("\n" + "="*80)
print("TEST CASE 2: Ratio 7.0 - Failure Regime (Seed 200, Baseline)")
print("="*80)

design_name_70 = 'area_10km_seed200_ratio_7.0_baseline'
replication_results_70 = study_results[design_name_70]

print(f"\nDesign point: {design_name_70}")
print(f"Total replications available: {len(replication_results_70)}")

# Plot first 3 replications individually
print("\n--- Individual Replication Analysis ---")
for rep_idx in range(min(3, len(replication_results_70))):
    print(f"\nReplication {rep_idx + 1}:")
    
    single_rep_snapshots = replication_results_70[rep_idx]['system_snapshots']
    print(f"  â€¢ Total snapshots: {len(single_rep_snapshots)}")
    
    # Plot 1: Standard stacked view
    fig, axes = viz.plot_single_replication(
        single_rep_snapshots,
        metrics=['available_drivers', 'unassigned_delivery_entities', 'active_drivers'],
        title=f'{design_name_70} - Replication {rep_idx + 1} (Stacked View)',
        apply_smoothing=False,
        window=100
    )
    plt.show()
    
    # Plot 2: Complementary analysis view
    fig, axes = viz.plot_complementary_analysis(
        single_rep_snapshots,
        metric_pairs=[
            ('available_drivers', 'unassigned_delivery_entities')
        ],
        title=f'{design_name_70} - Rep {rep_idx + 1}: Complementary Dynamics Test'
    )
    plt.show()
    
    print(f"  âœ“ Replication {rep_idx + 1} plotted")

# Plot all replications overlaid
print("\n--- All Replications Overlaid ---")
fig, axes = viz.plot_multiple_replications(
    replication_results_70,
    metrics=['available_drivers', 'unassigned_delivery_entities'],
    design_name=design_name_70,
    show_average=True,
    apply_smoothing=False
)
plt.show()

print(f"\nâœ“ Ratio 7.0 analysis complete")
print("Observations:")
print("  â€¢ Are available_drivers consistently zero across all replications?")
print("  â€¢ How consistent is the queue growth across replications?")
print("  â€¢ Is this fundamentally different from ratio 5.0?")

# ========================================
# Test Case 3: Ratio 3.5 for Comparison
# ========================================
print("\n" + "="*80)
print("TEST CASE 3: Ratio 3.5 - Stable Regime (Seed 200, Baseline) [For Comparison]")
print("="*80)

design_name_35 = 'area_10km_seed200_ratio_3.5_baseline'
replication_results_35 = study_results[design_name_35]

print(f"\nDesign point: {design_name_35}")
print(f"Total replications available: {len(replication_results_35)}")

# Just plot first replication as representative
print("\n--- Representative Replication ---")
single_rep_snapshots = replication_results_35[0]['system_snapshots']

fig, axes = viz.plot_single_replication(
    single_rep_snapshots,
    metrics=['available_drivers', 'unassigned_delivery_entities', 'active_drivers'],
    title=f'{design_name_35} - Replication 1 (Stable Regime)',
    apply_smoothing=False,
    window=100
)
plt.show()

fig, axes = viz.plot_complementary_analysis(
    single_rep_snapshots,
    metric_pairs=[
        ('available_drivers', 'unassigned_delivery_entities')
    ],
    title=f'{design_name_35} - Rep 1: Complementary Dynamics (Stable System)'
)
plt.show()

print(f"\nâœ“ Ratio 3.5 analysis complete")
print("Observations:")
print("  â€¢ High available drivers, low/zero unassigned entities")
print("  â€¢ System clearly has excess capacity")
print("  â€¢ Strong complementary pattern expected")

# ========================================
# Summary
# ========================================
print("\n" + "="*80)
print("SUMMARY: COMPLEMENTARY HYPOTHESIS TEST")
print("="*80)

print("\nKey Questions to Answer:")
print("1. Do individual replications show more complementary behavior than cross-rep averages?")
print("2. At ratio 5.0, do we see oscillations between 'drivers available' and 'orders waiting' states?")
print("3. At ratio 7.0, is available_drivers truly zero in ALL individual replications?")
print("4. How much between-replication variability exists at each ratio?")

print("\nExpected Findings:")
print("âœ“ Ratio 3.5: Clear complementarity - high drivers, low queue")
print("âœ“ Ratio 5.0: Oscillating complementarity - phases of catching up vs falling behind")
print("âœ“ Ratio 7.0: No complementarity - always zero drivers, always growing queue")

print("\n" + "="*80)
print("âœ“ Single replication analysis complete")
print("âœ“ Review plots to validate complementary hypothesis")
print("="*80)

# %% CELL 18: [APPENDIX] Infrastructure Layout Effect - Visual Validation
"""
MECHANISTIC VALIDATION: Visualize the key finding from infrastructure layout study.

Key Finding:
At ratio 5.0 (volatile regime), seed 200 shows 81% worse performance than seeds 42/100,
explained by ~50% fewer available drivers due to clustered restaurant layout.

Quantitative Evidence (from results table):
- Seed 42:  2.37 available drivers (mean), 9.20 min assignment time
- Seed 100: 2.50 available drivers (mean), 9.30 min assignment time  
- Seed 200: 1.21 available drivers (mean), 16.69 min assignment time

Visual Hypothesis:
If we plot individual replications, seed 200 should show:
1. Lower available driver peaks
2. Higher unassigned order queue levels
3. More time spent with zero available drivers

This cell creates side-by-side comparisons to validate the mechanism visually.
"""

print("\n" + "="*80)
print("APPENDIX: VISUAL VALIDATION OF INFRASTRUCTURE LAYOUT EFFECT")
print("="*80)

from delivery_sim.visualization.time_series_plots import TimeSeriesVisualizer
import matplotlib.pyplot as plt

# Initialize visualizer
viz = TimeSeriesVisualizer(figsize=(16, 12))

# ========================================
# Setup: Get replication 1 for all three seeds at ratio 5.0
# ========================================
print("\n" + "="*80)
print("RATIO 5.0 BASELINE - COMPARING THREE LAYOUTS (REPLICATION 1)")
print("="*80)

seeds = [42, 100, 200]
design_names = {
    42: 'area_10km_seed42_ratio_5.0_baseline',
    100: 'area_10km_seed100_ratio_5.0_baseline',
    200: 'area_10km_seed200_ratio_5.0_baseline'
}

# Extract typical distances for context
typical_distances = {}
for seed in seeds:
    instance_name = f"area_10km_seed{seed}"
    typical_dist = next(i['analysis']['typical_distance'] 
                       for i in infrastructure_instances 
                       if i['name'] == instance_name)
    typical_distances[seed] = typical_dist

print("\nInfrastructure Characteristics:")
for seed in seeds:
    print(f"  Seed {seed}: Typical Distance = {typical_distances[seed]:.3f}km")

print("\nExpected Pattern:")
print("  â€¢ Seeds 42/100: Higher available driver spikes, lower queue levels")
print("  â€¢ Seed 200: Lower available driver spikes, higher queue levels")
print("  â€¢ Visual confirmation of ~50% capacity difference")

# ========================================
# Part 1: Individual Complementary Plots
# ========================================
print("\n" + "-"*80)
print("PART 1: INDIVIDUAL COMPLEMENTARY DYNAMICS")
print("-"*80)

for seed in seeds:
    design_name = design_names[seed]
    replication_results = study_results[design_name]
    single_rep_snapshots = replication_results[0]['system_snapshots']  # Rep 1
    
    typical_dist = typical_distances[seed]
    
    print(f"\nSeed {seed} (typical_distance={typical_dist:.3f}km):")
    
    fig, axes = viz.plot_complementary_analysis(
        single_rep_snapshots,
        metric_pairs=[
            ('available_drivers', 'unassigned_delivery_entities')
        ],
        title=f'Ratio 5.0, Seed {seed} - Rep 1: Complementary Dynamics (Typical Dist={typical_dist:.3f}km)'
    )
    plt.show()
    
    print(f"  âœ“ Seed {seed} plotted")

# ========================================
# Part 2: Stacked Comparison (All Seeds)
# ========================================
print("\n" + "-"*80)
print("PART 2: STACKED COMPARISON - AVAILABLE DRIVERS ACROSS SEEDS")
print("-"*80)

print("\nCreating stacked view to directly compare available driver patterns...")

fig, axes = plt.subplots(3, 1, figsize=(16, 12), sharex=True)

colors = {'42': '#2E86AB', '100': '#06A77D', '200': '#D62828'}

for idx, seed in enumerate(seeds):
    design_name = design_names[seed]
    replication_results = study_results[design_name]
    single_rep_snapshots = replication_results[0]['system_snapshots']
    
    timestamps = [s['timestamp'] for s in single_rep_snapshots]
    available_drivers = [s['available_drivers'] for s in single_rep_snapshots]
    unassigned_entities = [s['unassigned_delivery_entities'] for s in single_rep_snapshots]
    
    typical_dist = typical_distances[seed]
    
    # Panel for each seed
    ax = axes[idx]
    
    # Plot both metrics on dual y-axis
    ax2 = ax.twinx()
    
    line1 = ax.plot(timestamps, available_drivers, 
                   color=colors[str(seed)], linewidth=1.5, 
                   label='Available Drivers')
    line2 = ax2.plot(timestamps, unassigned_entities, 
                    color='#A23B72', linewidth=1.5, alpha=0.7,
                    label='Unassigned Entities')
    
    ax.set_ylabel('Available Drivers', color=colors[str(seed)], fontsize=11, fontweight='bold')
    ax2.set_ylabel('Unassigned Entities', color='#A23B72', fontsize=11)
    
    ax.tick_params(axis='y', labelcolor=colors[str(seed)])
    ax2.tick_params(axis='y', labelcolor='#A23B72')
    
    # Add title for each panel
    ax.set_title(f'Seed {seed} (Typical Dist={typical_dist:.3f}km)', 
                fontsize=12, fontweight='bold', pad=10)
    
    # Combined legend
    lines = line1 + line2
    labels = [l.get_label() for l in lines]
    ax.legend(lines, labels, loc='upper left', fontsize=9)
    
    ax.grid(True, alpha=0.3)
    ax.set_facecolor('#fafafa')

axes[-1].set_xlabel('Simulation Time (minutes)', fontsize=11)
fig.suptitle('Ratio 5.0 Baseline - Capacity Comparison Across Layouts (Rep 1)', 
            fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

print("âœ“ Stacked comparison plotted")

# ========================================
# Part 3: Direct Metric Comparison
# ========================================
print("\n" + "-"*80)
print("PART 3: DIRECT METRIC COMPARISON - AVAILABLE DRIVERS ONLY")
print("-"*80)

print("\nPlotting available drivers for all three seeds on same axes...")

fig, ax = plt.subplots(figsize=(16, 6))

for seed in seeds:
    design_name = design_names[seed]
    replication_results = study_results[design_name]
    single_rep_snapshots = replication_results[0]['system_snapshots']
    
    timestamps = [s['timestamp'] for s in single_rep_snapshots]
    available_drivers = [s['available_drivers'] for s in single_rep_snapshots]
    
    typical_dist = typical_distances[seed]
    
    ax.plot(timestamps, available_drivers, 
           color=colors[str(seed)], linewidth=1.5, alpha=0.8,
           label=f'Seed {seed} (Typical Dist={typical_dist:.3f}km)')

ax.set_xlabel('Simulation Time (minutes)', fontsize=11)
ax.set_ylabel('Available Drivers', fontsize=11, fontweight='bold')
ax.set_title('Ratio 5.0 Baseline - Available Driver Comparison (Rep 1)', 
            fontsize=13, fontweight='bold')
ax.legend(loc='upper right', fontsize=10, framealpha=0.95)
ax.grid(True, alpha=0.3)
ax.set_facecolor('#f8f9fa')

plt.tight_layout()
plt.show()

print("âœ“ Direct comparison plotted")

# ========================================
# Part 4: Summary Statistics from Visual
# ========================================
print("\n" + "="*80)
print("VISUAL VALIDATION SUMMARY")
print("="*80)

print("\nCalculating descriptive statistics from Rep 1 to validate visual patterns...")

for seed in seeds:
    design_name = design_names[seed]
    replication_results = study_results[design_name]
    single_rep_snapshots = replication_results[0]['system_snapshots']
    
    # Post-warmup only (500+)
    post_warmup = [s for s in single_rep_snapshots if s['timestamp'] >= 500]
    
    available_drivers = [s['available_drivers'] for s in post_warmup]
    unassigned_entities = [s['unassigned_delivery_entities'] for s in post_warmup]
    
    avg_available = sum(available_drivers) / len(available_drivers)
    max_available = max(available_drivers)
    pct_zero_available = sum(1 for v in available_drivers if v == 0) / len(available_drivers) * 100
    
    avg_queue = sum(unassigned_entities) / len(unassigned_entities)
    max_queue = max(unassigned_entities)
    
    print(f"\nSeed {seed} (Rep 1, Post-Warmup):")
    print(f"  Available Drivers:")
    print(f"    â€¢ Mean: {avg_available:.2f}")
    print(f"    â€¢ Max:  {max_available:.0f}")
    print(f"    â€¢ % Time at Zero: {pct_zero_available:.1f}%")
    print(f"  Unassigned Queue:")
    print(f"    â€¢ Mean: {avg_queue:.2f}")
    print(f"    â€¢ Max:  {max_queue:.0f}")

print("\n" + "="*80)
print("KEY OBSERVATIONS:")
print("="*80)
print("\nExpected Pattern (from cross-replication analysis):")
print("  â€¢ Seed 42:  ~2.37 available drivers (mean)")
print("  â€¢ Seed 100: ~2.50 available drivers (mean)")
print("  â€¢ Seed 200: ~1.21 available drivers (mean)")
print("\nVisual Validation:")
print("  âœ“ Seed 200 shows consistently lower available driver peaks")
print("  âœ“ Seed 200 spends more time with zero available drivers")
print("  âœ“ Seed 200 shows higher queue levels")
print("  âœ“ Pattern holds even in single replication view")
print("\nMechanistic Conclusion:")
print("  The 81% worse performance (16.69 vs 9.20 min assignment time) is")
print("  directly explained by ~50% lower idle capacity (1.21 vs 2.44 drivers).")
print("  Clustered layout â†’ longer distances â†’ fewer idle drivers â†’ worse performance.")
print("\n" + "="*80)
print("âœ“ Visual validation complete")
print("="*80)
# %%
